# Итоговый проект №5

## Служба такси

<b>Выбор технологического стека</b>

Использован следующий стек технологий:

1. В качестве СУБД использован Postgres, поскольку таблица не требует высокой скорости чтения для аналитики и при расширении функционала БД службы такси она скорее будет ориентирована на запись, причем небольшого количества строк за раз.

2. Язык скриптов - python, библиотеки psycopg2 для подключения к БД и обмена данными и pandas для обработки данных датафреймами, а также выгрузки данных из БД в формате parquet.

3. Проект завернут в docker-compose.

Рассматривалась также возможность использвания hdfs, однако объем исходного файла небольшой (0,5 Гб) и формат данных столбцов определен достаточно точно из-за чего использование такого инструмента скорее излишне.

<i> Прим. из-за веса файлов с данными (соответственно сырые и обработанные а также слой витрин) и необходимости их обработки развертывание docker-compose требует большого количества свободной памяти (около 3GB или более)</i>

Проект представлен в двух вариантах: version1 (рабочий), version2 (~~теоретически~~ рабочий), принципиальных отличий нет, различается работа с данными core-слоя. В первой версии необходимо предварительно обработать данные для core-слоя и поместить их в нужную директорию, во второй -
весь процесс обработки и загрузки данных в БД происходит автоматически (требует большого объема свободной памяти).

<b>Работа с данными</b> 

<b>version1</b>

1. Требуеся предварительная подготовка файла с очищенными данными. Необходимо запустить скрипт [dataframe_processing](https://github.com/PolarJaba/DE_final/blob/main/version1/app/scripts/dataframe_processing.py) и поместить полученный файл в папку init_db/data. Здесь производится преобразование формата, проверка на существование всех необходимых значений строки, их адекватность.

2. При запуске docker-compose происходит загрузка данных в БД Postgres и создние витрины данных, а также ее выгрузка в формате parquet в отдельную директорию внутри контейнера.

<b>version2</b>

Работа осуществляется в несколько этапов:

1. Загрузка данных из файла yellow_tripdata в слой сырых данных без каких-либо изменений.

2. Получение части данных путем запроса к БД, для опимизации работы запрашиваются только отдельные столбцы, производится проверка на адекватность значений (например, расстояние или конечная стоимость путешествия не могут быть отрицательными величинами).

3. Преобразование полученных даннных в dataframe, конечная очистка.

4. Загрузка данных в core-слой.

5. Создание и заполнение витрины данных путем запроса к БД.

<b>Структура проекта</b>



